{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "tf.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "import os\n",
    "#os.environ['COLAB_SKIP_TPU_AUTH'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "def discount_reward(reward_sequence, gamma, normalization = False):\n",
    "    ans = [0]\n",
    "    for ind in reversed(range(len(reward_sequence))):\n",
    "        ans.append(reward_sequence[ind] + gamma*ans[-1])\n",
    "    ans = np.array(ans[1:][::-1])\n",
    "        \n",
    "    if normalization:\n",
    "        ans = ans - np.mean(ans)\n",
    "    return ans\n",
    "\n",
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\" \n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1) \n",
    "    image[image == 109] = 0 # erase background (background type 2) \n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1 \n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGgradient:\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "    def __init__(self, state_size,state_action_size, action_size, lr, qlr,name = 'PGNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.state_action_size = state_action_size\n",
    "        self.lr = lr\n",
    "        self.qlr = qlr\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name = 'input_')\n",
    "                self.q_inputs_ = tf.placeholder(tf.float32, [None, state_action_size], name = 'q_input_')\n",
    "                self.action = tf.placeholder(tf.float32, [None, action_size], name = 'acition')\n",
    "                self.dc_reward = tf.placeholder(tf.float32, shape = (None), name = 'discounted_reward')\n",
    "                \n",
    "                #this is the placeholder for predicted q-factor, !!!cannot use self.q_fc2, since this will\n",
    "                #update the parameter in the approximation coefficient as well \n",
    "                self.q_factor =tf.placeholder(tf.float32, shape = (None), name = 'q_factor')\n",
    "            \n",
    "            with tf.name_scope(\"Qfactor_fc\"):\n",
    "                self.q_fc1 = tf.layers.dense(inputs = self.q_inputs_, units = 64, \n",
    "                                           activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.initializers.glorot_uniform(),\n",
    "                                           name = 'qfc1') \n",
    "                self.q_fc2 = tf.layers.dense(inputs = self.q_fc1, units = 1, \n",
    "                                           activation = None,\n",
    "                                          kernel_initializer = tf.initializers.glorot_uniform(),\n",
    "                                           name = 'qfc2') \n",
    "                \n",
    "            with tf.name_scope(\"cov\"):\n",
    "                self.cov1 = tf.layers.conv2d(inputs = self.inputs_, filters = 32, kernel_size = [5,5],\n",
    "                                             strides = [3,3], padding = \"SAME\",\n",
    "                                              kernel_initializer = tf.initializers.glorot_uniform(),\n",
    "                                               name = 'conv1')\n",
    "                \n",
    "                self.cov1_batchnorm = tf.layers.batch_normalization(self.cov1, training = True,\n",
    "                                                                   epsilon=1e-5, name = 'batch_norm1')\n",
    "                \n",
    "                self.cov1_out = tf.nn.relu(self.cov1_batchnorm, name = 'conv1_out')\n",
    "                \n",
    "                \n",
    "                #self.cov2 = tf.layers.conv2d(inputs = self.cov1_out, filters = 16, kernel_size = [4,4],\n",
    "                #                             strides = [2,2], padding = \"SAME\",\n",
    "                #                         kernel_initializer = tf.initializers.glorot_uniform(),\n",
    "                #                           name = 'cov2')\n",
    "                \n",
    "                #self.cov2_batchnorm = tf.layers.batch_normalization(self.cov2, training = True,\n",
    "                #                                                   epsilon=1e-5, name = 'batch_norm2')\n",
    "                \n",
    "                #self.cov2_out = tf.nn.relu(self.cov2_batchnorm, name = 'conv2_out')\n",
    "                \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.layers.flatten(self.cov1_out)\n",
    "                \n",
    "            with tf.name_scope(\"fc\"):\n",
    "                \n",
    "                self.fc1 = tf.layers.dense(inputs = self.flatten, units = 64, \n",
    "                                           activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.initializers.glorot_uniform(),\n",
    "                                           name = 'fc2')\n",
    "                \n",
    "                \n",
    "                self.logits = tf.layers.dense(inputs = self.fc1, units = action_size, \n",
    "                                           activation = None,\n",
    "                                          kernel_initializer = tf.initializers.glorot_uniform(),\n",
    "                                           name = 'logits')\n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distri = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.logProbStateAction = tf.nn.softmax_cross_entropy_with_logits(logits = self.logits,\n",
    "                                                                           labels = self.action)\n",
    "\n",
    "                \n",
    "                self.negative_weighted_logProb = tf.multiply(self.logProbStateAction, self.dc_reward - self.q_factor)\n",
    "                \n",
    "                self.negative_object = tf.reduce_mean(self.negative_weighted_logProb)\n",
    "                \n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "                self.train_opt = self.optimizer.minimize(self.negative_object)\n",
    "            \n",
    "            \n",
    "        \n",
    "            with tf.name_scope(\"Qfactor_loss\"):\n",
    "                #print(self.logProbStateAction)\n",
    "                self.q_loss = tf.nn.l2_loss(self.q_fc2 - self.dc_reward, name = 'l2loss')\n",
    "                \n",
    "                self.q_object = tf.reduce_mean(self.q_loss)\n",
    "                \n",
    "            with tf.name_scope(\"Qfactor_train\"):\n",
    "                self.q_optimizer = tf.train.RMSPropOptimizer(self.qlr)\n",
    "                self.q_train_opt = self.optimizer.minimize(self.q_object)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_simu(batch_size, pg, gamma):# this is indeed the sample function from envir\n",
    "    states = []\n",
    "    action = []\n",
    "    reward_episode = []\n",
    "    reward_seq = []\n",
    "    discounted_reward_seq = np.array([])\n",
    "    state_action_seq = []\n",
    "    state = preprocess(env.reset())\n",
    "    batch = 0\n",
    "    num_episode = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action_prob_distri = sess.run(pg.action_distri, feed_dict ={pg.inputs_:[state]})\n",
    "        \n",
    "        action_taken = np.random.choice(range(pg.action_size), p = action_prob_distri[0])\n",
    "        \n",
    "        action_onehot = np.zeros(pg.action_size)\n",
    "        action_onehot[action_taken] = 1\n",
    "        \n",
    "        states.append(state)\n",
    "        action.append(action_onehot) \n",
    "        state_action_seq.append(np.concatenate((state.reshape((-1,1)),action_onehot), axis = None))\n",
    "\n",
    "        next_state, reward, done, info = env.step(action_taken+2)\n",
    "        next_state = preprocess(next_state)\n",
    "        reward_episode.append(reward)\n",
    "\n",
    "        \n",
    "    \n",
    "        if not done:\n",
    "            \n",
    "            \n",
    "            state = next_state\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            state = preprocess(env.reset())\n",
    "            reward_seq += reward_episode\n",
    "            discounted_reward_seq = np.concatenate((discounted_reward_seq,\n",
    "                                                   discount_reward(reward_episode, gamma, True)))\n",
    "            \n",
    "            batch += len(reward_episode)\n",
    "            \n",
    "            reward_episode = []\n",
    "            \n",
    "            num_episode += 1\n",
    "            \n",
    "            if batch > batch_size:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    return np.array(states), np.array(action), np.array(reward_seq), discounted_reward_seq, num_episode, np.array(state_action_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =  gym.make('Pong-v0')\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pg = PGgradient(state_size= [80, 80,1],state_action_size = 6402, action_size=2, lr = 0.0001, qlr = 0.2)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "Epoch:  20 / 2000\n",
      "Number of training episode with this batch:  2\n",
      "Mean Reward for episode with this batch:  -20.0\n",
      "==================================\n",
      "Epoch:  40 / 2000\n",
      "Number of training episode with this batch:  2\n",
      "Mean Reward for episode with this batch:  -21.0\n",
      "==================================\n",
      "Epoch:  60 / 2000\n",
      "Number of training episode with this batch:  2\n",
      "Mean Reward for episode with this batch:  -21.0\n",
      "==================================\n",
      "Epoch:  80 / 2000\n",
      "Number of training episode with this batch:  2\n",
      "Mean Reward for episode with this batch:  -20.5\n",
      "==================================\n",
      "Epoch:  100 / 2000\n",
      "Number of training episode with this batch:  2\n",
      "Mean Reward for episode with this batch:  -20.5\n",
      "==================================\n",
      "Epoch:  120 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -18.0\n",
      "==================================\n",
      "Epoch:  140 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -21.0\n",
      "==================================\n",
      "Epoch:  160 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -21.0\n",
      "==================================\n",
      "Epoch:  180 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -19.0\n",
      "==================================\n",
      "Epoch:  200 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  220 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  240 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  260 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  280 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  300 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  320 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  340 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -16.0\n",
      "==================================\n",
      "Epoch:  360 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  380 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  400 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -18.0\n",
      "==================================\n",
      "Epoch:  420 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  440 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  460 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  480 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -10.0\n",
      "==================================\n",
      "Epoch:  500 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -7.0\n",
      "==================================\n",
      "Epoch:  520 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  540 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  560 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -8.0\n",
      "==================================\n",
      "Epoch:  580 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  600 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  620 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -8.0\n",
      "==================================\n",
      "Epoch:  640 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  660 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  680 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  700 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  720 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  740 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -16.0\n",
      "==================================\n",
      "Epoch:  760 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  780 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  800 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  820 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  840 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  860 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  880 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  900 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -5.0\n",
      "==================================\n",
      "Epoch:  920 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  940 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  960 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  980 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  1000 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1020 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -8.0\n",
      "==================================\n",
      "Epoch:  1040 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1060 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -6.0\n",
      "==================================\n",
      "Epoch:  1080 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -7.0\n",
      "==================================\n",
      "Epoch:  1100 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1120 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "Epoch:  1140 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  1160 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1180 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  1200 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  1220 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1240 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1260 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1280 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  1300 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -6.0\n",
      "==================================\n",
      "Epoch:  1320 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1340 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1360 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1380 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -16.0\n",
      "==================================\n",
      "Epoch:  1400 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  1420 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1440 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1460 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -8.0\n",
      "==================================\n",
      "Epoch:  1480 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1500 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -18.0\n",
      "==================================\n",
      "Epoch:  1520 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1540 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  1560 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  1580 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  1600 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -10.0\n",
      "==================================\n",
      "Epoch:  1620 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  1640 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -8.0\n",
      "==================================\n",
      "Epoch:  1660 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  1680 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  1700 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1720 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  1740 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  1760 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1780 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1800 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -20.0\n",
      "==================================\n",
      "Epoch:  1820 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1840 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -13.0\n",
      "==================================\n",
      "Epoch:  1860 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n",
      "==================================\n",
      "Epoch:  1880 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1900 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -15.0\n",
      "==================================\n",
      "Epoch:  1920 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -9.0\n",
      "==================================\n",
      "Epoch:  1940 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -11.0\n",
      "==================================\n",
      "Epoch:  1960 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -14.0\n",
      "==================================\n",
      "Epoch:  1980 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -17.0\n",
      "==================================\n",
      "Epoch:  2000 / 2000\n",
      "Number of training episode with this batch:  1\n",
      "Mean Reward for episode with this batch:  -12.0\n"
     ]
    }
   ],
   "source": [
    "#this is the main training code:\n",
    "\n",
    "\n",
    "#the discounting rate\n",
    "gamma = 0.99\n",
    "batch_size = 2000\n",
    "num_epoches = 2000\n",
    "mean_reward = []\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "while epoch < num_epoches+1:\n",
    "    \n",
    "    #simulate batch\n",
    "    states, action, reward_seq, discounted_reward_seq, num_episode, state_action_seq= batch_simu(batch_size, pg, gamma)\n",
    "    meanRewardEpoch = sum(reward_seq)/num_episode\n",
    "    mean_reward.append(meanRewardEpoch)\n",
    "    \n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"==================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epoches)\n",
    "        print(\"Number of training episode with this batch: \", num_episode)\n",
    "        print(\"Mean Reward for episode with this batch: \", meanRewardEpoch)\n",
    "    \n",
    "    #update the Q-factor\n",
    "    cur_loss = float('inf')\n",
    "    q_train_epoch = 0\n",
    "    while True:\n",
    "         \n",
    "        next_loss = sess.run([pg.q_object], feed_dict = {pg.q_inputs_:state_action_seq,\n",
    "                                            pg.dc_reward: discounted_reward_seq})\n",
    "                \n",
    "        if abs(next_loss[0] - cur_loss) < 1e-5:\n",
    "            break\n",
    "        \n",
    "        sess.run([pg.q_train_opt],  feed_dict = {pg.q_inputs_:state_action_seq,\n",
    "                                            pg.dc_reward: discounted_reward_seq})\n",
    "        \n",
    "        q_train_epoch += 1\n",
    "        cur_loss = next_loss\n",
    "        \n",
    "        \n",
    "    #print(\"Training Q-factor epoch: \"+ str(q_train_epoch) +'; loss: '+ str(cur_loss))\n",
    "    \n",
    "    #get the q factor estimate for the current batch of states\n",
    "    temp = sess.run([pg.q_fc2],  feed_dict = {pg.q_inputs_:state_action_seq})\n",
    "    \n",
    "    #run the policy gradient\n",
    "    sess.run([pg.train_opt], feed_dict = {pg.inputs_: states, pg.q_inputs_:state_action_seq,\n",
    "                                          pg.action: action, pg.dc_reward: discounted_reward_seq,\n",
    "                                         pg.q_factor: temp}) \n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.plot(range(len(mean_reward)), mean_reward)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
